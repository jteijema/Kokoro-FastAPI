services:
  model-fetcher:
    image: datamachines/git-lfs:latest
    environment:
      - SKIP_MODEL_FETCH=${SKIP_MODEL_FETCH:-false}
    volumes:
      - ./Kokoro-82M:/app/Kokoro-82M
    working_dir: /app/Kokoro-82M
    command: >
      sh -c "
        if [ \"$$SKIP_MODEL_FETCH\" = \"true\" ]; then
          echo 'Skipping model fetch...' && touch .cloned;
        else
          rm -f .git/index.lock;
          if [ -z \"$(ls -A .)\" ]; then
            git clone https://huggingface.co/hexgrad/Kokoro-82M .
            touch .cloned;
          else
            rm -f .git/index.lock && \
            git checkout main && \
            git pull origin main && \
            touch .cloned;
          fi;
        fi;
        tail -f /dev/null
      "
    healthcheck:
      test: ["CMD", "test", "-f", ".cloned"]
      interval: 5s
      timeout: 2s
      retries: 300
      start_period: 1s

  kokoro-tts:
    # image: ghcr.io/remsky/kokoro-fastapi:latest
    # Uncomment below to build from source instead of using the released image
    build:
      context: .
    volumes:
      - ./api/src:/app/api/src
      - ./Kokoro-82M:/app/Kokoro-82M
    ports:
      - "8880:8880"
    environment:
      - PYTHONPATH=/app:/app/Kokoro-82M
      # Resource Management  
      - CLEAR_CUDA_CACHE=true # Clear CUDA cache after warmup/each completed request
      - N_CACHE_VOICES = 5 # Cache 1 voice
      - N_WARMUPS=1
      # TTS Settings
      - MAX_CHUNK_SIZE=300 # Max is 500 for best quality; lower values improve latency to a point
      - GAP_TRIM_MS=250 # Trim silence between chunks for more natural speaking flow

    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    depends_on:
      model-fetcher:
        condition: service_healthy

  # Gradio UI service [Comment out everything below if you don't need it]
  gradio-ui:
    # image: ghcr.io/remsky/kokoro-fastapi:latest-ui
    # Uncomment below to build from source instead of using the released image
    build:
      context: ./ui
    ports:
      - "7860:7860"
    volumes:
      - ./ui/data:/app/ui/data
      - ./ui/app.py:/app/app.py  # Mount app.py for hot reload
    environment:
      - GRADIO_WATCH=True  # Enable hot reloading
